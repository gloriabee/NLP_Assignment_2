{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6229b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install py-readability-metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6ffcc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ExPertComputer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ExPertComputer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from readability import Readability\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fb5e2b",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55298a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names=['index','timestamp','query','query_type','username','text']\n",
    "test_df=pd.read_csv('test.csv',encoding='latin1',names=column_names)\n",
    "train_df=pd.read_csv('train.csv',encoding='latin1',names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38bc302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9365dbfb",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e42017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    original_length=len(text)\n",
    "    text=re.sub(r'[^a-zA-Z\\s]','',text) \n",
    "    text=re.sub(r'\\s+',' ',text).strip()\n",
    "    cleaned_length=len(text)\n",
    "    return text,original_length-cleaned_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab0421e",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bf37cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    sentences=sent_tokenize(text)\n",
    "    words=word_tokenize(text)\n",
    "    return sentences,words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c7553",
   "metadata": {},
   "source": [
    "### Lowercasing and Stopword removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8de25750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tokens(tokens):\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    tokens_lower=[word.lower() for word in tokens]\n",
    "    filtered_tokens=[word for word in tokens_lower if word not in stop_words]\n",
    "    return filtered_tokens,len(tokens)-len(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42be4a42",
   "metadata": {},
   "source": [
    "### Emoticons, Stemmming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7c63454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_advanced(tokens):\n",
    "    stemmer=PorterStemmer()\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    stems=[stemmer.stem(word) for word in tokens]\n",
    "    lemmas=[lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return stems,lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c459d376",
   "metadata": {},
   "source": [
    "### Phone number, account, and address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a6185bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_sensitive_info(text):\n",
    "    address_pattern=r'\\d+\\s+[a-zA-Z]+\\s+(Street|St|Avenue|Ave|Road|Rd|Lane|Ln|Boulevard|Blvd|Drive|Dr)'\n",
    "    phone_pattern=r'\\b\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b'\n",
    "    account_pattern=r'\\b\\d{10,}\\b'\n",
    "    addresses=re.findall(address_pattern,text)\n",
    "    phones=re.findall(phone_pattern,text)\n",
    "    accounts=re.findall(account_pattern,text)\n",
    "    return len(addresses),len(phones),len(accounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8e31f7",
   "metadata": {},
   "source": [
    "### Statistics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dbc5207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(text,tokens,sentences):\n",
    "    unique_words=set(tokens)\n",
    "    sentence_lengths=[len(word_tokenize(sent)) for sent in sentences]\n",
    "    return {\n",
    "        'average_sentence_length': sum(sentence_lengths)/len(sentences),\n",
    "        'word_count':len(tokens),\n",
    "        'sentence_count':len(sentences),\n",
    "        'vocabulary_size':len(unique_words),\n",
    "        'max_word_length':max(len(word) for word in tokens),\n",
    "        'min_sentence_length':min(sentence_lengths),\n",
    "        'max_sentence_length':max(sentence_lengths)\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8d7a63",
   "metadata": {},
   "source": [
    "### Automated Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78486a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(text):\n",
    "    if len(word_tokenize(text)) < 100:\n",
    "        return None, None  # Default values for short texts\n",
    "    r = Readability(text)\n",
    "    fk = r.flesch_kincaid()\n",
    "    lexical_diversity = len(set(word_tokenize(text))) / len(word_tokenize(text))\n",
    "    return fk.score, lexical_diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722d1708",
   "metadata": {},
   "source": [
    "### After Cleaning Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4efa1abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Initialize metrics\n",
    "    special_chars_removed = 0\n",
    "    stop_words_removed = 0\n",
    "    addresses_detected = 0\n",
    "    phones_detected = 0\n",
    "    accounts_detected = 0\n",
    "\n",
    "    # Initialize stats for before and after cleaning\n",
    "    before_stats = {\n",
    "        'doc_count': len(data),\n",
    "        'word_count': 0,\n",
    "        'sentence_count': 0,\n",
    "        'vocabulary_size': 0,\n",
    "        'max_word_length': 0,\n",
    "        'min_sentence_length': float('inf'),\n",
    "        'max_sentence_length': 0,\n",
    "        'avg_sentence_length': 0\n",
    "    }\n",
    "    after_stats = before_stats.copy()\n",
    "    vocabulary = Counter()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    results = []\n",
    "    for index, row in data.iterrows():\n",
    "        text = row['text']\n",
    "\n",
    "        # Step 1: Collect before-cleaning stats\n",
    "        sentences, tokens = tokenize_text(text)\n",
    "        before_stats['word_count'] += len(tokens)\n",
    "        before_stats['sentence_count'] += len(sentences)\n",
    "        vocabulary.update(tokens)\n",
    "        before_stats['max_word_length'] = max(before_stats['max_word_length'], max(len(word) for word in tokens))\n",
    "        sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
    "        before_stats['min_sentence_length'] = min(before_stats['min_sentence_length'], min(sentence_lengths, default=0))\n",
    "        before_stats['max_sentence_length'] = max(before_stats['max_sentence_length'], max(sentence_lengths, default=0))\n",
    "\n",
    "        # Step 2: Clean Text\n",
    "        cleaned_text, chars_removed = clean_text(text)\n",
    "        special_chars_removed += chars_removed\n",
    "\n",
    "        # Skip processing if text has fewer than 100 words\n",
    "        if len(word_tokenize(cleaned_text)) < 100:\n",
    "            results.append({\n",
    "                'cleaned_text': cleaned_text,\n",
    "                'stats': None,\n",
    "                'readability': None,\n",
    "                'lexical_diversity': None\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Step 3: Detect sensitive info\n",
    "        addr_count, phone_count, acc_count = detect_sensitive_info(text)\n",
    "        addresses_detected += addr_count\n",
    "        phones_detected += phone_count\n",
    "        accounts_detected += acc_count\n",
    "\n",
    "        # Step 4: Process tokens after cleaning\n",
    "        sentences, tokens = tokenize_text(cleaned_text)\n",
    "        tokens_processed, stops_removed = process_tokens(tokens)\n",
    "        stop_words_removed += stops_removed\n",
    "        vocabulary.update(tokens_processed)\n",
    "\n",
    "        # Collect after-cleaning stats\n",
    "        after_stats['word_count'] += len(tokens_processed)\n",
    "        after_stats['sentence_count'] += len(sentences)\n",
    "        after_stats['max_word_length'] = max(after_stats['max_word_length'], max(len(word) for word in tokens_processed))\n",
    "        sentence_lengths = [len(sentence.split()) for sentence in sentences]\n",
    "        after_stats['min_sentence_length'] = min(after_stats['min_sentence_length'], min(sentence_lengths, default=0))\n",
    "        after_stats['max_sentence_length'] = max(after_stats['max_sentence_length'], max(sentence_lengths, default=0))\n",
    "\n",
    "        stats = calculate_statistics(cleaned_text, tokens_processed, sentences)\n",
    "        readability, lexical_diversity = evaluate_metrics(cleaned_text)\n",
    "\n",
    "        results.append({\n",
    "            'cleaned_text': cleaned_text,\n",
    "            'stats': stats,\n",
    "            'readability': readability,\n",
    "            'lexical_diversity': lexical_diversity\n",
    "        })\n",
    "\n",
    "    # Calculate vocabulary size\n",
    "    before_stats['vocabulary_size'] = len(vocabulary.keys())\n",
    "    after_stats['vocabulary_size'] = len(vocabulary.keys())\n",
    "\n",
    "    # Calculate average sentence length\n",
    "    before_stats['avg_sentence_length'] = before_stats['word_count'] / max(before_stats['sentence_count'], 1)\n",
    "    after_stats['avg_sentence_length'] = after_stats['word_count'] / max(after_stats['sentence_count'], 1)\n",
    "\n",
    "    # Total runtime\n",
    "    runtime = (time.time() - start_time) / 60  # Convert seconds to minutes\n",
    "\n",
    "    # Return results and metrics\n",
    "    cleaning_metrics = {\n",
    "        'special_chars_removed': special_chars_removed,\n",
    "        'stop_words_removed': stop_words_removed,\n",
    "        'addresses_detected': addresses_detected,\n",
    "        'phones_detected': phones_detected,\n",
    "        'accounts_detected': accounts_detected\n",
    "    }\n",
    "    return results, before_stats, after_stats, cleaning_metrics, runtime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a48dbc-c071-4f49-b797-97728a88c09e",
   "metadata": {},
   "source": [
    "### Output Display Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ce7d3-63e1-40ad-83e7-9fe7dd1fd762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cleaning_summary(before_stats, after_stats, cleaning_metrics, runtime):\n",
    "    # Generate the formatted output\n",
    "    output = f\"\"\"\n",
    "### Text Cleaning Statistics ###\n",
    "\n",
    "- Number of documents: {before_stats['doc_count']} → {after_stats['doc_count']}\n",
    "- Average sentence length: {before_stats['avg_sentence_length']:.1f} → {after_stats['avg_sentence_length']:.1f}\n",
    "- Word count: {before_stats['word_count']} → {after_stats['word_count']}\n",
    "- Sentence count: {before_stats['sentence_count']} → {after_stats['sentence_count']}\n",
    "- Vocabulary size: {before_stats['vocabulary_size']} → {after_stats['vocabulary_size']}\n",
    "- Max word length: {before_stats['max_word_length']} → {after_stats['max_word_length']}\n",
    "- Min sentence length: {before_stats['min_sentence_length']} → {after_stats['min_sentence_length']}\n",
    "- Max sentence length: {before_stats['max_sentence_length']} → {after_stats['max_sentence_length']}\n",
    "- Special characters removed: {cleaning_metrics['special_chars_removed']}\n",
    "- Stop words removed: {cleaning_metrics['stop_words_removed']}\n",
    "- Addresses detected: {cleaning_metrics['addresses_detected']}\n",
    "- Phone numbers detected: {cleaning_metrics['phones_detected']}\n",
    "- Account numbers detected: {cleaning_metrics['accounts_detected']}\n",
    "- Total runtime: {runtime:.1f} minutes\n",
    "    \"\"\"\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb577e0",
   "metadata": {},
   "source": [
    "### Final Result of trained data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1861519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `data` is a DataFrame with a 'text' column\n",
    "results, before_stats, after_stats, cleaning_metrics, runtime = preprocess_data(train_df)\n",
    "display_cleaning_summary(before_stats, after_stats, cleaning_metrics, runtime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d71e3db-94cc-4f1c-9bc8-86d2fc12eee2",
   "metadata": {},
   "source": [
    "### Final Result of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa6627-403f-4f22-a23c-cd9255f053dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `data` is a DataFrame with a 'text' column\n",
    "results, before_stats, after_stats, cleaning_metrics, runtime = preprocess_data(test_df)\n",
    "display_cleaning_summary(before_stats, after_stats, cleaning_metrics, runtime)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
